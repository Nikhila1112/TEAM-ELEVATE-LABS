import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

# Load Titanic dataset
df = pd.read_csv("Titanic-Dataset.csv") 

# Step 1: Basic Cleaning

# Drop irrelevant columns
df.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'], inplace=True)

# Handle missing values
# Fill Age with median
df['Age'].fillna(df['Age'].median(), inplace=True)
# Fill Embarked with most frequent value
df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)

# Step 2: Encode categorical columns


# Convert 'Sex' and 'Embarked' to dummy variables
df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)

# Step 3: Split features and target


X = df.drop(columns=['Survived'])
y = df['Survived']


# Step 4: Feature Scaling


scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 5: Train-test split

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

print("Titanic data cleaned, encoded, scaled, and split successfully!")
